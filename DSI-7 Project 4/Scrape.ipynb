{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING WITH BEAUTIFULSOUP AND SELENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ACTIVE VERSION\n",
    "\n",
    "# SECTION 1: read in existing file\n",
    "consolidated_urls = pd.read_csv('./Data/D2/consolidated_urls.csv')\n",
    "\n",
    "\n",
    "# SECTION 2: define search terms (add/edit if necessary)\n",
    "search_terms = ['data scientist', 'data analyst', 'business analyst', 'business intelligence', 'data architect',\\\n",
    "                'data engineer','database engineer', 'research scientist', 'data governance', 'data manager',\\\n",
    "                'python developer']\n",
    "\n",
    "\n",
    "# SECTION 3: create empty df to append urls\n",
    "all_urls = pd.DataFrame()\n",
    "\n",
    "\n",
    "# SECTION 4: iterate through search terms\n",
    "for term in search_terms:\n",
    "\n",
    "    # SECTION 4.1: TOTAL NUMBER OF JOBS TO ITERATE THROUGH (FOR THE WHILE LOOP THAT FOLLOWS)\n",
    "\n",
    "    # get the total number of jobs in a search\n",
    "    counter = 0\n",
    "    path = 'https://www.mycareersfuture.sg/search?search=' + term + '&sortBy=new_posting_date&page='\n",
    "    page = str(counter)\n",
    "    path_page = str(path + page)\n",
    "\n",
    "    # initialise browser\n",
    "    browser = webdriver.Chrome()\n",
    "\n",
    "    # navigate to url\n",
    "    browser.get(path_page)\n",
    "\n",
    "    sleep(5)\n",
    "\n",
    "    # get the source code as string\n",
    "    source = browser.page_source\n",
    "\n",
    "    # parsing the string-type html to proper html format\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    # get the number of jobs\n",
    "    total_jobs = int([val.text for val in soup.findAll('span',{'class':'pl2 pl0-ns f5 black-70 fw4 db lh-copy'})][0].split(\" \")[0])\n",
    "\n",
    "    # close browser\n",
    "    browser.close()\n",
    "\n",
    "\n",
    "    ## SECTION 4.2: CODE TO ITERATE THROUGH THE JOBS TO GET THE URLS\n",
    "\n",
    "    job_urls = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while len(job_urls) < total_jobs:\n",
    "        \n",
    "        # filepath/url of each page\n",
    "        path = 'https://www.mycareersfuture.sg/search?search=' + term + '&sortBy=new_posting_date&page='\n",
    "        page = str(counter)\n",
    "        path_page = path + page\n",
    "\n",
    "        # initialise browser\n",
    "        browser = webdriver.Chrome()\n",
    "\n",
    "        # navigate to url\n",
    "        browser.get(path_page)\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        # this code gets the source code as string\n",
    "        source = browser.page_source\n",
    "\n",
    "        # parsing the string-type html to proper html format\n",
    "        soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        # get the element where the href exists\n",
    "\n",
    "        rel_urls = []\n",
    "\n",
    "        for item in soup.findAll('a', {'class': 'bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3'}, href=True):\n",
    "            rel_urls.append(item['href'])\n",
    "\n",
    "\n",
    "        # append the standard url format in front of the href link\n",
    "        for item in rel_urls:\n",
    "            link = 'https://www.mycareersfuture.sg' + item\n",
    "            job_urls.append(link)\n",
    "\n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "    # parse into dataframe and append to all_urls\n",
    "    temp_df = pd.DataFrame(job_urls)\n",
    "\n",
    "    temp_df['search_type'] = term\n",
    "    \n",
    "    all_urls = all_urls.append(temp_df)\n",
    "\n",
    "# SECTION 5: rename all_urls columns and remove duplicates\n",
    "all_urls = all_urls.rename(columns={0:'url'})\n",
    "all_urls.drop_duplicates('url', inplace=True)\n",
    "\n",
    "# SECTION 6: append to consolidated_urls and export to csv\n",
    "consolidated_urls = consolidated_urls.append(all_urls)\n",
    "consolidated_urls.drop_duplicates('url', inplace=True)\n",
    "\n",
    "consolidated_urls.to_csv('./Data/D2/consolidated_urls.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many unique urls\n",
    "some_url = consolidated_urls.head()\n",
    "some_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrape Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_jobs = []\n",
    "\n",
    "for link in consolidated_urls['url']:\n",
    "\n",
    "    # initialise browser\n",
    "    browser = webdriver.Chrome()\n",
    "\n",
    "    # navigate to url\n",
    "    browser.get(link)\n",
    "\n",
    "    sleep(5)\n",
    "\n",
    "    # get the source code as string\n",
    "    source = browser.page_source\n",
    "\n",
    "    # parsing the string-type html to proper html format\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    browser.close()\n",
    "\n",
    "\n",
    "\n",
    "    job = []\n",
    "\n",
    "    # company = []\n",
    "    for item in soup.findAll('p', {'name': 'company'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # job_title = []\n",
    "    for item in soup.findAll('h1', {'id': 'job_title'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # job_id = []\n",
    "    for item in soup.findAll('span', {'class': 'black-60 db f6 fw4 mv1'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # job_type = []\n",
    "    for item in soup.findAll('p', {'id': 'employment_type'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    salary = []\n",
    "    for item in soup.findAll('span', {'class': 'dib'}):\n",
    "        salary.append(item.text)\n",
    "    salary_range = salary[1]\n",
    "    job.append(salary_range)\n",
    "    salary_freq = salary[-2]\n",
    "    job.append(salary_freq)\n",
    "\n",
    "    # level = []\n",
    "    for item in soup.findAll('p', {'id': 'seniority'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # industry = []\n",
    "    for item in soup.findAll('p', {'id': 'job-categories'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # job_description = []\n",
    "    for item in soup.findAll('div', {'id': 'job_description'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # requirements = []\n",
    "    for item in soup.findAll('div', {'id': 'requirements'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    all_jobs.append(job)\n",
    "\n",
    "\n",
    "columns = ['company', 'job_title', 'job_id', 'job_type', 'salary_range', 'salary_freq', 'level', 'industry',\\\n",
    "           'job_description', 'requirements']\n",
    "\n",
    "job_df = pd.DataFrame(all_jobs, columns=columns)\n",
    "job_df.to_csv('./Data/D2/job_data.csv', index=False)\n",
    "job_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated version where i did second round of scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT LIBRARIES -------------------------------------------\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SCRAPE URLs -------------------------------------------------\n",
    "\n",
    "# SECTION 1: read in existing file\n",
    "consolidated_urls = pd.read_csv('./Data/consolidated_urls.csv')\n",
    "\n",
    "\n",
    "# SECTION 2: define search terms (add/edit if necessary)\n",
    "search_terms = ['data specialist', 'data scientist', 'data analyst', 'business analyst', 'business intelligence',\\\n",
    "                'data architect', 'data engineer','database engineer', 'research scientist', 'data governance',\\\n",
    "                'data manager', 'python developer']\n",
    "\n",
    "\n",
    "# SECTION 3: create empty df to store urls\n",
    "all_urls = pd.DataFrame()\n",
    "\n",
    "\n",
    "# SECTION 4: iterate through search terms\n",
    "for term in search_terms:\n",
    "\n",
    "    # SECTION 4.1: GET TOTAL NUMBER OF JOBS TO ITERATE THROUGH (FOR THE WHILE LOOP THAT FOLLOWS)\n",
    "\n",
    "    # get the total number of jobs in a search\n",
    "    counter = 0\n",
    "    path = 'https://www.mycareersfuture.sg/search?search=' + term + '&sortBy=new_posting_date&page='\n",
    "    page = str(counter)\n",
    "    path_page = str(path + page)\n",
    "\n",
    "    # initialise browser\n",
    "    browser = webdriver.Chrome()\n",
    "\n",
    "    # navigate to url\n",
    "    browser.get(path_page)\n",
    "\n",
    "    sleep(5) # allow time for page to load\n",
    "\n",
    "    # get the source code as string\n",
    "    source = browser.page_source\n",
    "\n",
    "    # parsing the string-type html to proper html format\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    # get the number of jobs\n",
    "    total_jobs = int([val.text for val in soup.findAll('span',{'class':'pl2 pl0-ns f5 black-70 fw4 db lh-copy'})][0].split(\" \")[0])\n",
    "\n",
    "    # close browser\n",
    "    # browser.close()\n",
    "\n",
    "\n",
    "\n",
    "    ## SECTION 4.2: START TO ITERATE THROUGH THE JOBS TO GET THE URLS\n",
    "\n",
    "    job_urls = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while len(job_urls) < total_jobs: # loop will continue until the number of urls hit the total number of expected jobs found in each search term\n",
    "        \n",
    "        # filepath/url of each page\n",
    "        path = 'https://www.mycareersfuture.sg/search?search=' + term + '&sortBy=new_posting_date&page='\n",
    "        page = str(counter)\n",
    "        path_page = path + page\n",
    "\n",
    "        # initialise browser\n",
    "        # browser = webdriver.Chrome()\n",
    "\n",
    "        # navigate to url\n",
    "        browser.get(path_page)\n",
    "\n",
    "        sleep(5) # allow time for page to load\n",
    "\n",
    "        # this code retrieves the source code as string\n",
    "        source = browser.page_source\n",
    "\n",
    "        # parsing the string-type html to proper html format\n",
    "        soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        # get the element where the href (url) exists\n",
    "\n",
    "        rel_urls = []\n",
    "\n",
    "        for item in soup.findAll('a', {'class': 'bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3'}, href=True):\n",
    "            rel_urls.append(item['href'])\n",
    "\n",
    "\n",
    "        # append the standard url format in front of the href link\n",
    "        for item in rel_urls:\n",
    "            link = 'https://www.mycareersfuture.sg' + item\n",
    "            job_urls.append(link)\n",
    "\n",
    "\n",
    "        counter = counter + 1 # once completed, +1 to move to the next page\n",
    "\n",
    "        # browser.close()\n",
    "\n",
    "    # parse into dataframe and append to all_urls\n",
    "    temp_df = pd.DataFrame(job_urls)\n",
    "\n",
    "    temp_df['search_type'] = term # add a column so we know which search term each url is found under\n",
    "    \n",
    "    all_urls = all_urls.append(temp_df)\n",
    "\n",
    "# SECTION 5: rename all_urls columns and remove duplicates\n",
    "all_urls = all_urls.rename(columns={0:'url'})\n",
    "all_urls.drop_duplicates('url', inplace=True)\n",
    "\n",
    "# SECTION 6: append to consolidated_urls, drop duplicates and finally export to csv\n",
    "consolidated_urls = consolidated_urls.append(all_urls)\n",
    "consolidated_urls.drop_duplicates('url', inplace=True)\n",
    "\n",
    "consolidated_urls.to_csv('./Data/consolidated_urls.csv', index=False, date_format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SCRAPE DETAILS INSIDE EACH URL--------------------------------------------\n",
    "\n",
    "all_jobs = []\n",
    "\n",
    "for link in consolidated_urls['url']: # iterate through each of the URLs\n",
    "\n",
    "    # initialise browser\n",
    "    # browser = webdriver.Chrome()\n",
    "\n",
    "    # navigate to url\n",
    "    browser.get(link)\n",
    "\n",
    "    sleep(5)\n",
    "\n",
    "    # get the source code as string\n",
    "    source = browser.page_source\n",
    "\n",
    "    # parsing the string-type html to proper html format\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    # browser.close()\n",
    "\n",
    "\n",
    "\n",
    "    job = []\n",
    "\n",
    "    # get company name\n",
    "    for item in soup.findAll('p', {'name': 'company'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # get job title\n",
    "    for item in soup.findAll('h1', {'id': 'job_title'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # get job id\n",
    "    for item in soup.findAll('span', {'class': 'black-60 db f6 fw4 mv1'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # get job type\n",
    "    for item in soup.findAll('p', {'id': 'employment_type'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # get salary (freq + range)\n",
    "    salary = []\n",
    "    for item in soup.findAll('span', {'class': 'dib'}):\n",
    "        salary.append(item.text)\n",
    "    salary_range = salary[1]\n",
    "    job.append(salary_range)\n",
    "    salary_freq = salary[-2]\n",
    "    job.append(salary_freq)\n",
    "\n",
    "    # get job level\n",
    "    for item in soup.findAll('p', {'id': 'seniority'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # get industry\n",
    "    for item in soup.findAll('p', {'id': 'job-categories'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # get job description\n",
    "    for item in soup.findAll('div', {'id': 'job_description'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    # get job requirement\n",
    "    for item in soup.findAll('div', {'id': 'requirements'}):\n",
    "        job.append(item.text)\n",
    "\n",
    "    all_jobs.append(job)\n",
    "\n",
    "# close browser\n",
    "browser.close()\n",
    "\n",
    "\n",
    "\n",
    "# define column names\n",
    "columns = ['company', 'job_title', 'job_id', 'job_type', 'salary_range', 'salary_freq', 'level', 'industry',\\\n",
    "           'job_description', 'requirements']\n",
    "\n",
    "# put everything into a dataframe before exporting to csv\n",
    "job_df = pd.DataFrame(all_jobs, columns=columns)\n",
    "job_df.to_csv('./Data/job_data.csv', index=False, date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
